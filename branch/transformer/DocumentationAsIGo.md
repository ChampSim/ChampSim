## Positional Encoder

Positional encoders encode the relative position of an incoming token __*ip in our case*__. This is needed to prevent the transformer from attenuating to irrelvant or out-of-sequence tokens, ensuring a sequential nature to it's thinking. "" *I'm here now, I came from here, therefore I will go there* "" 


Normally, positional encoders have a maximum, you can see this in max context window size. However, since we're implimenting this into a cpu we need a system which can "wrap" but still maintain correct psoitional awareness. 


#### Fixed-range positional encoding with modulo arithmetic:

By introducing a mechansim to disambiguate overlapping positions we can still work with a fixed range (e.g: 0 to 10,000 - 1) with a modulo operation.

**Approach:**
- **Modulo encoding:** Use the positional encoding for (pos mod 10,000)
- **Disambiguation:** Introduce a small secondary feature, (counter or embeddeding) to indicate wrap-number

**Formula:**

$$PE(pos)=Sinusoidal((pos mod 10,000)) + WrapEmbedding(⌊pos / 10,000⌋)$$

- **Sinusoidal((pos mod 10,000))**: Standard sin encoding with fixed range
- **WrapEmbedding(|pos / 10,000|)**: Small learnable vector or constant offset for each wrap. 

**Advantages:** Mod is easy on hardware & wrap counter adds minimal complexity.


## Options (GPT provided) 


If you're constrained to **5 bits for positional encoding**, and your application is for a **branch predictor** with access to the **current instruction pointer (IP)** and **24 previous instructions**, your options for positional encoding are naturally constrained by these limitations. However, leveraging existing techniques like those used in **hashed perceptrons** or **TAGE predictors** can work. Below are options that fit your constraints:

---

### **1. Relative Position Encoding Using History Differences**
Relative position encoding focuses on **distance-based information**. Instead of encoding absolute positions, it encodes the difference between the current instruction pointer and each previous instruction.

- **How It Works:**
  - For each instruction in the 24-history window, calculate the difference between the current IP and the previous IPs.
  - Modulo these differences by \(2^5 = 32\) to fit the 5-bit constraint.
  - Encode the differences into the 5-bit positional field.

- **Implementation Example:**

  ```cpp
  std::vector<uint8_t> positionEncoding(uint64_t current_ip, const std::vector<uint64_t>& history) {
      std::vector<uint8_t> encoding;
      for (const auto& ip : history) {
          uint8_t relative_pos = static_cast<uint8_t>((current_ip - ip) % 32); // Modulo to fit 5 bits
          encoding.push_back(relative_pos);
      }
      return encoding; // Returns a vector of 24 5-bit positional values
  }
  ```

- **Benefits:**
  - No ambiguity within the 24-instruction history window.
  - Compact and hardware-efficient, since modulo and subtraction are simple operations.
  - Captures the relative position, which is key for many predictors like TAGE.
  
- **Drawbacks:**
  - Positional ambiguity occurs if the absolute difference modulo \(2^5\) repeats for instructions far apart (unlikely for nearby branches).

---

### **2. Hashed Positional Encoding Using Instruction Pointer and Global History**
You can hash the **current IP** together with the **global branch history** (e.g., the last 24 branch outcomes) to generate a **5-bit hash-based position index**. This approach emulates hashed perceptrons.

- **How It Works:**
  - Combine the lower bits of the instruction pointer (e.g., 12 least significant bits) with the global branch history using a simple hash function (e.g., XOR or CRC).
  - Modulo the result by \(2^5\) to get a 5-bit positional encoding.

- **Implementation Example:**

  ```cpp
  uint8_t hashPosition(uint64_t current_ip, uint32_t global_history) {
      // Combine instruction pointer and global history
      uint64_t hash_input = (current_ip & 0xFFF) ^ global_history; // Use 12 LSBs of IP
      return static_cast<uint8_t>(hash_input % 32); // Reduce to 5 bits
  }
  ```

- **Benefits:**
  - Simple and hardware-efficient.
  - Avoids direct dependence on absolute position; instead, leverages instruction behavior and history.
  - Fits well into existing branch predictor pipelines like hashed perceptrons.

- **Drawbacks:**
  - Possibility of hash collisions, though typically rare and acceptable.

---

### **3. Reuse TAGE-Like Geometric History Lengths**
TAGE uses **tagged tables** indexed by a combination of **global history bits** and the **instruction pointer**. You can adapt this by encoding positions based on the **geometric history lengths** used in TAGE, which capture patterns at varying distances.

- **How It Works:**
  - Divide the 24-history window into smaller geometric segments (e.g., 4, 8, 16).
  - Encode each segment's "relative" position modulo \(2^5 = 32\).
  - Combine these encodings into a 5-bit representation for each segment.

- **Implementation Example:**

  ```cpp
  std::vector<uint8_t> geometricPositionEncoding(uint64_t current_ip, const std::vector<uint64_t>& history) {
      std::vector<int> segment_lengths = {4, 8, 16}; // Example lengths
      std::vector<uint8_t> encoding;

      for (int len : segment_lengths) {
          uint64_t segment_ip = history[std::max(0, static_cast<int>(history.size()) - len)];
          uint8_t relative_pos = static_cast<uint8_t>((current_ip - segment_ip) % 32); // Modulo to fit 5 bits
          encoding.push_back(relative_pos);
      }

      return encoding; // Encodes relative positions for geometric lengths
  }
  ```

- **Benefits:**
  - Exploits varying history lengths, as TAGE does, to capture both short- and long-term patterns.
  - Encodes relative information efficiently with modulo arithmetic.
  - Avoids absolute position dependence.

- **Drawbacks:**
  - Slightly more complex than hashing, as it requires geometric segmentation.

---

### **4. Precomputed Cyclic Positional Encoding**
If the absolute position repeats cyclically (as it does with 5 bits), precompute a **cyclic positional encoding table** for all 32 possible positions. Each position maps to a unique embedding or encoding. As new instructions arrive, you compute their relative position within the cycle and use the precomputed embedding.

- **How It Works:**
  - Precompute a table of 32 embeddings (5 bits each).
  - For each incoming instruction, calculate its position modulo 32 and use the corresponding embedding.

- **Implementation Example:**

  ```cpp
  std::vector<uint8_t> precomputed_table = {/* Precomputed 5-bit values */};

  uint8_t cyclicPositionEncoding(uint64_t current_ip, const std::vector<uint64_t>& history) {
      uint8_t relative_pos = static_cast<uint8_t>(current_ip % 32); // Cyclic position
      return precomputed_table[relative_pos]; // Lookup in precomputed table
  }
  ```

- **Benefits:**
  - Extremely fast and hardware-efficient (no computation beyond modulo).
  - Precomputed embeddings ensure unique representations within each cycle.

- **Drawbacks:**
  - Cycles every 32 instructions, introducing ambiguity for distant instructions.

---

### **Comparison of Approaches**

| **Approach**                     | **Key Idea**                                                                 | **Benefits**                                      | **Drawbacks**                                      |
|-----------------------------------|-------------------------------------------------------------------------------|--------------------------------------------------|---------------------------------------------------|
| **Relative Position Encoding**    | Encode distance between current IP and history modulo 32                     | Simple, avoids absolute dependence               | Ambiguity for distant branches                    |
| **Hash-Based Encoding**           | Hash current IP with global history and reduce modulo 32                     | Compact, avoids global counters                  | Hash collisions                                   |
| **TAGE-Like Geometric Encoding**  | Use relative positions with varying geometric history lengths                | Captures short- and long-term patterns           | Slightly more complex implementation              |
| **Cyclic Precomputed Encoding**   | Use precomputed 5-bit cyclic embeddings                                      | Simple, hardware-efficient                       | Cycles every 32 steps, ambiguity for repetitions  |

---

### **Recommendation**

If your goal is:
- **Short-Term Positional Accuracy:** Use **relative positional encoding** or **geometric encoding**.
- **Compact and Fast:** Use **hash-based encoding** or **cyclic precomputed encoding**.

Given your constraints of **5 bits** and **branch prediction** with 24-history, **relative positional encoding modulo 32** is likely the best balance of simplicity, efficiency, and functionality for your use case.




# Attention

## Q, K, V Matricies
### Size:
Generally, 
$$ W_Q,W_K,W_V \in \R^{{d_{model}} \times (h \cdot d_k)} \\
\text{\sf where } {d_k=d_{model}/d_h}
$$

However, modern techniques *(like those used in gpt)* reduce model size by seperating W_v's overall parameter count to equal:
$$
\text{Value Param Count} = \text{Query Params} + \text{Key Params}
$$
